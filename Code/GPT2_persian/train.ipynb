{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b452e6-48ab-44a0-934e-9af8fe4f2e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: F:\\projects\\python_notebook\\myenv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary F:\\projects\\python_notebook\\myenv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965c4c5a-96f8-4ff6-8759-09c418c7655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer path\n",
    "save_path = 'tokenized_data'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")# creating the model\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce04bad-4920-4bc3-8eab-02078b628d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text files (articles)\n",
    "paths = [str(x) for x in Path(\"./fa_corpus/\").glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfe4bc7-1765-48d6-8b8e-6529413cc752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0049a66-73bf-4ee1-aa4a-2a29b0c7c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths[:274] #for test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f04d70-592f-4dde-95c7-1045fd449569",
   "metadata": {},
   "source": [
    "# Creating Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5d43e2-4764-488e-b138-0b53c195764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string =''\n",
    "for filename in paths:\n",
    "  with open(filename, \"r\", encoding='utf-8') as f:\n",
    "   x = f.read()\n",
    "  single_string += x + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dcad7c-3e4c-406c-82eb-5ea5ec9d47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(input_data), torch.tensor(label)\n",
    "        # return input_data, label\n",
    "\n",
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "\n",
    "inputs, labels = [], []\n",
    "\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b73fe1-18b0-45d4-ac8f-d912ad27f13e",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab4aad1-a03d-4f2b-bf15-36307b7e3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1005275-1511-4bf9-9153-c24191a4bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer):\n",
    "\n",
    "    for i in tqdm.tqdm(range(EPOCHS)):\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(X, labels=X).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        ## save model to directory\n",
    "        output_dir = './model/'# creating directory if it is not present\n",
    "        if not os.path.exists(output_dir):\n",
    "          os.mkdir(output_dir)\n",
    "        torch.save(model.state_dict(), f\"{output_dir}model_fa.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f057c715-0dc0-4872-9c1b-d32649d83f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [03:42<00:00, 22.24s/it]\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4d7d6-6aaa-48fc-b9e5-5c373768dec1",
   "metadata": {},
   "source": [
    "# Testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c12b5cf-66fb-491e-b74a-80ff5ebabcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "    # todo model in infernence mod\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(\n",
    "        X, \n",
    "        attention_mask=a,\n",
    "        max_length = 50,\n",
    "        num_beams = 5,\n",
    "        temperature = 0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=5)\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output\n",
    "\n",
    "\n",
    "# from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "# output_dir = './model_bn_custom/'# creating directory if it is not present\n",
    "# if not os.path.exists(output_dir):\n",
    "#   os.mkdir(output_dir)\n",
    "# model_to_save = model.module if hasattr(model, 'module') else modeloutput_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "# output_config_file = os.path.join(output_dir, CONFIG_NAME)# save model and model configs\n",
    "# model.save_pretrained(output_dir)\n",
    "# model_to_save.config.to_json_file(output_config_file)# save tokenizer\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "# model = TFGPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9600600d-c804-47d1-b3ce-52519773b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'جستجو های یافت شده در اثر این اثر در تاریخ با شماره ثبت به عنوان یکی از آثار ملی ایران به ثبت رسیده است جستارهای وابسته فهرست آثار دوره قاجاریان در شهرستان سازمان میراث فرهنگی صنایع دستی گردشگری منابع</s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"جستجو های یافت شده در\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68620d04-a77a-40e9-b67b-f1e12aed11c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
