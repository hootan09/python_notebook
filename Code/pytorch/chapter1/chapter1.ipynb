{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da9ac40-5697-4596-84c6-87320dd25e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83933b99-db84-49ba-83c3-fa1feaf377e3",
   "metadata": {},
   "source": [
    "#### In this chapter, we will:\n",
    "- briefly review the steps of gradient descent (optional)\n",
    "- use gradient descent to implement a linear regression in Numpy\n",
    "- create tensors in PyTorch (finally!)\n",
    "- understand the difference between CPU and GPU tensors\n",
    "- understand PyTorch’s main feature, autograd, to perform automatic\n",
    "differentiation\n",
    "- create a loss function\n",
    "- define an optimizer\n",
    "- implement our own model class\n",
    "- implement nested and sequential models, using PyTorch’s layers\n",
    "- organize our code into three parts: data preparation, model configuration and\n",
    "model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a647347-5601-4e3e-a04e-77f11a26d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[ 1.2434,  0.8843, -0.1261,  0.3930],\n",
      "         [-0.8288, -0.2430, -1.0502, -1.0295],\n",
      "         [-0.6323, -0.3248, -0.5428,  1.4598]],\n",
      "\n",
      "        [[-0.1730, -0.9990,  0.0602,  1.0524],\n",
      "         [-0.0621,  0.3701, -0.4136, -0.7180],\n",
      "         [ 0.0387,  1.8177, -1.1853,  0.2027]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca23218-bcac-44ac-ba57-f0bf9de39c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n",
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)\n",
    "print(scalar.size(), scalar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2d98b0-f26f-4b12-bda4-c9d0b74a4a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# We get a tensor with a different shape but it still is\n",
    "# the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d4cc3e-2680-4707-b31c-5cff6e160ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niki\\AppData\\Local\\Temp\\ipykernel_5468\\823975569.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  different_matrix = matrix.new_tensor(matrix.view(1, 6))\n"
     ]
    }
   ],
   "source": [
    "# We can use \"new_tensor\" method to REALLY copy it into a new one\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\n",
    "# Now, if we change one of its elements...\n",
    "different_matrix[0, 1] = 3.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "# But we get a \"warning\" from PyTorch telling us\n",
    "# to use \"clone()\" instead!\n",
    "print(matrix)\n",
    "print(different_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe1c656-7c95-4360-ae82-43a61490689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print(another_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6495939a-3cd9-46a1-950f-e5677a434668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N= 80\n",
    "x = np.random.rand(N, 1)\n",
    "true_b = 1\n",
    "true_w = 2\n",
    "epsilon = 0.01\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "\n",
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "x_train.dtype, x_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6e7ff9-d7e2-4094-88b1-573842fd0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = x_train_tensor.float()\n",
    "float_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63ffa855-b3d0-4581-a184-f459481092a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Tensor gets modified too...\n",
    "dummy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78383f1-9765-482d-98b6-bababa2c529c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57c9c8fd-f919-415d-9d01-064cc680bf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89cd02ac-1d28-4fc2-87e0-205c8889740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "n_cudas = torch.cuda.device_count()\n",
    "print(n_cudas)\n",
    "\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a0a47b-d8b3-48e9-a68b-56d450c4a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7470], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = torch.as_tensor(x_train).to(device)\n",
    "gpu_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a914e371-8fae-4348-baf0-4a49bea700c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Our data was in Numpy arrays, but we need to transform them\n",
    "# into PyTorch's Tensors and then we send them to the\n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "253d27e6-2179-4eab-a131-2fff3b69430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the difference - notice that .type() is more\n",
    "# useful since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bbf3450-35e0-4cc0-ad73-ca9f9c8aee3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m back_to_numpy \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "back_to_numpy = x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1782dda1-39e6-4bfc-8575-d8bbd316d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_numpy = x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f1c32ca-b26c-44bc-891c-8dc2aceaa96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we\n",
    "# did in Numpy since we want to apply gradient descent on\n",
    "# these parameters we need to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efc66a54-1b65-4503-a003-b08d7591ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([0.1288], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just\n",
    "# send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185dd95f-d530-42e2-a64e-46dfd9c837b5",
   "metadata": {},
   "source": [
    "We succeeded in sending them to another device, but we ”lost” the gradients\n",
    "somehow, since there is no more requires_grad=True, (don’t bother the weird\n",
    "grad_fn). Clearly, we need to do better…\n",
    "In the third chunk, we first send our tensors to the device and then use\n",
    "requires_grad_() method to set its requires_grad attribute to True in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2915bef7-094d-4762-b9c5-bf5f51fca945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], device='cuda:0', requires_grad=True) tensor([0.1288], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to\n",
    "# the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae591c35-fac5-498f-9771-38287dfc6b84",
   "metadata": {},
   "source": [
    "In PyTorch, every method that ends with an underscore ( _ ), like\n",
    "the requires_grad_() method above, makes changes in-place,\n",
    "meaning, they will modify the underlying variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bae9ff2a-bba3-42f9-8e5d-077510f6ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FINAL\n",
    "# We can specify the device at the moment of creation\n",
    "# RECOMMENDED!\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dcc05-8abd-4c63-98a0-93866561e640",
   "metadata": {},
   "source": [
    "#### Autograd\n",
    "Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need\n",
    "to worry about partial derivatives, chain rule, or anything like it.\n",
    "#### backward\n",
    "So, how do we tell PyTorch to do its thing and compute all gradients? That’s the\n",
    "role of the backward() method. It will compute gradients for all (requiring gradient)\n",
    "tensors involved in the computation of a given variable.\n",
    "Do you remember the starting point for computing the gradients? It was the loss,\n",
    "as we computed its partial derivatives w.r.t. our parameters. Hence, we need to\n",
    "invoke the backward() method from the corresponding Python variable:\n",
    "```loss.backward().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5429773a-c1c9-4f85-bdea-68b30bd50407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "# Step 2 - Computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient\n",
    "# descent. How wrong is our model? That's the error!\n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "# No more manual computation of gradients!\n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_tensor * error).mean()\n",
    "loss.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe989e34-9034-4594-9ffe-094728767d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(error.requires_grad, yhat.requires_grad, \\\n",
    "b.requires_grad, w.requires_grad)\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7af958b3-8edc-4572-9a93-0f9b68068323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.0809], device='cuda:0') tensor([-4.2031], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165bbe-8cab-4faa-9c28-964d6f0e928e",
   "metadata": {},
   "source": [
    "OK, but that is actually a problem: we need to use the gradients corresponding to\n",
    "the current loss to perform the parameter update. We should NOT use\n",
    "accumulated gradients.\n",
    "\"If accumulating gradients is a problem, why does PyTorch do it by default?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe13ec9-b6b1-48bf-a55e-ecec168c5bea",
   "metadata": {},
   "source": [
    "it turns out; this behavior can be useful to circumvent hardware limitations.\n",
    "During the training of large models, the necessary number of data points in a mini-\n",
    "batch may be too big to fit in memory (of the graphics card). How to solve this,\n",
    "other than buying more expensive hardware?\n",
    "One can split a mini-batch into \"sub-mini-batches\" (horrible name, I know, don’t\n",
    "quote me on this!), compute the gradients for those \"sub\" and accumulate them to\n",
    "achieve the same result of computing the gradients on the full mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4737c-9e2a-4538-ba59-31dbc5ba7a4b",
   "metadata": {},
   "source": [
    "#### zero_\n",
    "Every time we use the gradients to update the parameters, we need to zero the\n",
    "gradients afterward. And that’s what zero_() is good for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a334bce7-8a0a-4623-98a7-1f36b150f20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will be placed _after_ Step 4\n",
    "# (updating the parameters)\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab5779a0-7be1-4b8a-b3c4-da8a1b2a9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0100], device='cuda:0', requires_grad=True) tensor([2.0000], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error!\n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\"\n",
    "    # parameters. No more manual computation of gradients!\n",
    "    # b_grad = 2 * error.mean()\n",
    "    # w_grad = 2 * (x_tensor * error).mean()\n",
    "    # We just tell PyTorch to work its way BACKWARDS\n",
    "    # from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate. But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad\n",
    "    # w = w - lr * w.grad\n",
    "    # print(b)\n",
    "    \n",
    "    # SECOND ATTEMPT - using in-place Python assingment\n",
    "    # RuntimeError: a leaf Variable that requires grad\n",
    "    # has been used in an in-place operation.\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad\n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of\n",
    "    # the gradient computation. Why is that? It boils\n",
    "    # down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we\n",
    "    # need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597379cd-8ea9-409b-875d-2b10e63041da",
   "metadata": {},
   "source": [
    "### Adding Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bf17b00-2018-4131-b16a-68a6f48d69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0100], device='cuda:0', requires_grad=True) tensor([2.0000], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error!\n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate. No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    # b -= lr * b.grad\n",
    "    # w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "\n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9d5d400-005b-48fc-8648-c734019ac3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean') #'mean' or 'sum'\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bca1f36e-6a97-4c4d-9bef-d3eb983fbc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1700)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a random example to illustrate the loss function\n",
    "predictions = torch.tensor([0.5, 1.0])\n",
    "labels = torch.tensor([2.0, 1.3])\n",
    "loss_fn(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98b1db-96b4-486e-a68e-ed2cdfe5db47",
   "metadata": {},
   "source": [
    "### Using loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0a78a2a-8208-4764-b6d7-3f2caf7692c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0100], device='cuda:0', requires_grad=True) tensor([2.0000], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = (yhat - y_train_tensor)\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99703dc9-79be-4267-8096-b859ccfb38df",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7aef3aa0-7bf8-4e1b-b7b4-860528cf7c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.063061e-12, dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3b7021a-3ae6-469f-8d18-e771d7910a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.063061081500564e-12 5.063061081500564e-12\n"
     ]
    }
   ],
   "source": [
    "#or using\n",
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fc247-dfdb-46d5-ae32-76f4c2e3bdad",
   "metadata": {},
   "source": [
    "### Model\n",
    "In PyTorch, a model is represented by a regular Python class that inherits from the\n",
    "Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c6f475f-df92-4d5c-9ae4-204ada94302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model,\n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1,\n",
    "                            requires_grad=True,\n",
    "                            dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1,\n",
    "                            requires_grad=True,\n",
    "                            dtype=torch.float))\n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bea4b3dc-b230-4b86-a78c-21e22f2673e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12814d94-ffbe-429a-8f21-274d55895504",
   "metadata": {},
   "source": [
    "we can use our model’s parameters()\n",
    "method to retrieve an iterator over all model’s parameters, including parameters\n",
    "of nested models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50312a02-2d32-44f9-a301-25e87b759527",
   "metadata": {},
   "source": [
    "### state_dict\n",
    "Moreover, we can get the current values of all parameters using our model’s\n",
    "state_dict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d25ed0f-2821-4d51-a007-8907d6f4782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b925ccd-3ef2-4859-adee-23f82eedfda8",
   "metadata": {},
   "source": [
    "The state_dict() of a given model is simply a Python dictionary that maps each\n",
    "attribute/parameter to its corresponding tensor. But only ```learnable``` parameters\n",
    "are included, as its purpose is to keep track of parameters that are going to be\n",
    "updated by the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57054555-d952-4a21-8e5d-417815e2053f",
   "metadata": {},
   "source": [
    "By the way, the optimizer itself has a state_dict() too, which contains its internal\n",
    "state, as well as other hyper-parameters. Let’s take a quick look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dcf90969-6d3e-41d9-b0a5-9f56eb550917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'params': [0, 1]}]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bd31a-fefb-4ba6-8467-adc9d4846d34",
   "metadata": {},
   "source": [
    "IMPORTANT: we need to send our model to the same device\n",
    "where the data is. If our data is made of GPU tensors, our model\n",
    "must “live” inside the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69e355ee-1b3f-4bf7-8c1c-d768eba56529",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "# and sends it to the device\n",
    "dummy = ManualLinearRegression().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b10d26-5291-4e9a-9d6c-3d354253012d",
   "metadata": {},
   "source": [
    "DO NOT call model.forward(x)!\n",
    "Otherwise, your model’s hooks will not work (if you have them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "146c0568-c7d6-4b63-b1f7-60dedbff6cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0100], device='cuda:0')), ('w', tensor([2.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor) \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e26f2-7704-4542-92a2-ea7e2bd229ed",
   "metadata": {},
   "source": [
    "### Nested Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a927cb85-da93-451b-8393-037ef11e8e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1, out_features=1, bias=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1)\n",
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0924750b-4878-47c0-8cb3-5857ad6b2a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[-0.4869]])), ('bias', tensor([0.5873]))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b268a5-db5a-45e0-a3cd-9eaa5a41ab6c",
   "metadata": {},
   "source": [
    "#### Building Model using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "706350e5-9f62-4399-9c49-06b1d564fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model\n",
    "        # with a single input and a single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b2521a7-01ee-4e4d-a3d9-5d446296d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ef2c11d-7e84-47a3-b21e-61ac1885a489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182c41a-a8a1-4344-98e7-6015e8051bc1",
   "metadata": {},
   "source": [
    "### Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bdff1f7-8f04-4d66-9265-7eca9aff3aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c37384-2d04-4476-87a5-3057b20c56f1",
   "metadata": {},
   "source": [
    "### layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a598adf3-a9f0-4312-b26c-b546e7512d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4414,  0.4792, -0.1353],\n",
       "                      [ 0.5304, -0.1265,  0.1165],\n",
       "                      [-0.2811,  0.3391,  0.5090],\n",
       "                      [-0.4236,  0.5018,  0.1081],\n",
       "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
       "             ('1.bias', tensor([-0.2062], device='cuda:0'))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a3d20eb-17db-4a75-9f07-f6af0ae26cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer1): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also use a model’s add_module() method to be able to name the layers:\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential()\n",
    "model.add_module('layer1', nn.Linear(3, 5))\n",
    "model.add_module('layer2', nn.Linear(5, 1))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46626506-b4e9-4a63-b64a-5beeb2678a98",
   "metadata": {},
   "source": [
    "#### There are MANY different layers that can be used in PyTorch:\n",
    "- Convolution Layers\n",
    "- Pooling Layers\n",
    "- Padding Layers\n",
    "- Non-linear Activations\n",
    "- Normalization Layers\n",
    "- Recurrent Layers\n",
    "- Transformer Layers\n",
    "- Linear Layers\n",
    "- Dropout Layers\n",
    "- Sparse Layers (embeddings)\n",
    "- Vision Layers\n",
    "- DataParallel Layers (multi-GPU)\n",
    "- Flatten Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54b4f8-1c59-4e46-b4d5-3650e3568205",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc6cad-0abc-4cfd-af7e-6aa1dbfc23d5",
   "metadata": {},
   "source": [
    "it is time to put it all together and organize our code so far into three fundamental\n",
    "parts, namely:\n",
    "- #### data preparation (not data generation!)\n",
    "- #### model configuration\n",
    "- #### model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f4982-1b95-4a1d-8ada-adeb816c79f0",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e678b098-874c-4b16-9930-4c64cdf98137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparation/v0.py\n",
    "#prepare data\n",
    "N= 80\n",
    "x = np.random.rand(N, 1)\n",
    "true_b = 1\n",
    "true_w = 2\n",
    "y = true_b + true_w * x\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Our data was in Numpy arrays, but we need to transform them\n",
    "# into PyTorch's Tensors and then we send them to the\n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c4ae5ca-a0c3-4a76-bf1f-1ea3910396d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparation/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278b24e-a2e3-4818-aa46-ea95804ab4d6",
   "metadata": {},
   "source": [
    "- %%writefile:\n",
    "  \n",
    "    as its name says, it writes the contents of the cell to a file,\n",
    "    but it does not run it, so we need to use yet another magic…\n",
    "\n",
    "- %run:\n",
    "  \n",
    "    it runs the named file inside the notebook as a program - but\n",
    "    independently from the rest of the notebook, so we need to use the -i\n",
    "    option to make all variables available, both from the notebook and the\n",
    "    file (technically speaking, the file is executed in IPython’s namespace).\n",
    "\n",
    "In a nutshell, a cell containing one of our three fundamental parts will be\n",
    "written to a versioned file inside the folder corresponding to that part.\n",
    "In the example above, we write the cell to the data_preparation folder,\n",
    "name it v0.py and then execute it using the %run -i magic.\n",
    "\n",
    "all command for run:\n",
    "\n",
    "- %run -i data_preparation/v0.py\n",
    "- %run -i model_configuration/v1.py\n",
    "- %run -i model_training/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0c051-8830-4c82-a84f-28349eb559d4",
   "metadata": {},
   "source": [
    "#### Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef3357-55c5-422e-8456-dfa949dc7023",
   "metadata": {},
   "source": [
    "- a model\n",
    "- a loss function (which needs to be chosen according to your model)\n",
    "- an optimizer (although some people may disagree with this choice, it makes it\n",
    "easier for further organizing the code…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8385a66-f51c-459a-98c8-c6b3c98bf52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v0.py\n",
    "\n",
    "# This is redundant now, but it won't be when we introduce\n",
    "# Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39190417-b381-4ab4-ab9c-e747f92d564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46301e4-e8f4-4276-8270-eda4f74b84bd",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c04206-f9cb-423c-9be7-a829e99ea4ee",
   "metadata": {},
   "source": [
    "- Step 1: compute model’s predictions\n",
    "- Step 2: compute the loss\n",
    "- Step 3: compute the gradients\n",
    "- Step 4: update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d7447-1e9d-43d8-9fb2-0ebb23bd00eb",
   "metadata": {},
   "source": [
    "This sequence is repeated over and over until the number of epochs is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e351463-8423-4cbc-a9b2-10780e0cfb87",
   "metadata": {},
   "source": [
    "What happened to the random initialization step?\n",
    "\n",
    "Since we are not manually creating parameters anymore, the initialization is\n",
    "handled inside each layer during model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "148d5f3e-677d-49ff-94e1-7952276540bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v0.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "    \n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "34cad31e-8d42-498b-afa0-02b98ae8f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "599667c2-5553-4283-bbdc-bc7c6ce89ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0100], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2dedd-2abe-44a1-a422-4c2da3657449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
