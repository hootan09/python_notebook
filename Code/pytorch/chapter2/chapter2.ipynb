{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781a025b-53f1-4b80-a403-6818f63218a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a85401-59a1-48d9-9700-ee4bd3d5cf98",
   "metadata": {},
   "source": [
    "#### In this chapter, we will:\n",
    "- build a function to perform training steps\n",
    "- implement our own dataset class\n",
    "- use data loaders to generate mini-batches\n",
    "- build a function to perform mini-batch gradient descent\n",
    "- evaluate our model\n",
    "- integrate TensorBoard to monitor model training\n",
    "- save/checkpoint our model to disk\n",
    "- load our model from disk to resume training or to deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1851d10-03d1-4d28-9e84-3d46ca50c9a7",
   "metadata": {},
   "source": [
    "### We finished the previous chapter with an important question:\n",
    "\n",
    "**Would the code inside the training loop change if we were using a\n",
    "different optimizer, or loss, or even model?**\n",
    "\n",
    "The answer: **NO.**\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd815eeb-da35-482c-b4ff-3d0d2656b565",
   "metadata": {},
   "source": [
    "**A function that returns another function?**\n",
    "\n",
    "Sounds complicated, right? \n",
    "\n",
    "It is not as bad as it sounds, though… that’s called a\n",
    "**higher-order function**, and it is very useful for reducing boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b613c3-1c88-440b-a056-6c27e3a2e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentiation_builder(exponent):\n",
    "    def skeleton_exponentiation(x):\n",
    "        return x ** exponent\n",
    "    return skeleton_exponentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131dac84-8cf9-42ac-a162-d62994b3ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.exponentiation_builder.<locals>.skeleton_exponentiation(x)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_function = exponentiation_builder(2)\n",
    "returned_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a053d481-66b9-4643-8dbe-d211be88f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_function(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726cf35-f5a2-4e13-bf72-8052fb24310c",
   "metadata": {},
   "source": [
    "## Train (working with high order functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90562411-d09b-4391-91ad-f095c8721a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1 - Computes model's predictions - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3 - Computes gradients for \"b\" and \"w\" parameters\n",
    "        loss.backward()\n",
    "        # Step 4 - Updates parameters using gradients and\n",
    "        # the learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    # Returns the function that will be called inside the\n",
    "    # train loop\n",
    "    return perform_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a99d7a-4592-4610-acca-a4a9fe4cf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a03b91-48e2-4c51-9866-9e0f1c1f2141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../chapter1/model_configuration/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_configuration/v1.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Creates the train_step function for our model, loss function\n",
    "# and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb411e56-1cbd-40fb-ae43-d1f6d063d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_configuration/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1287405-c2e5-4b97-b351-74caa83ba4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_train_step.<locals>.perform_train_step(x, y)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402e04ea-e71e-49d8-bf60-55871ff05bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../chapter1/model_training/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_training/v1.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3fab8a8-7fd2-467b-9237-dc4192ce2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb5eef2e-810b-4812-96c7-08017c6c714f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7778562307357788,\n",
       " 0.4567660987377167,\n",
       " 0.27874717116355896,\n",
       " 0.1797574758529663,\n",
       " 0.12442906945943832,\n",
       " 0.09322968125343323,\n",
       " 0.07537204027175903,\n",
       " 0.06489849090576172,\n",
       " 0.058518461883068085,\n",
       " 0.054414354264736176,\n",
       " 0.0515819787979126,\n",
       " 0.04946642741560936,\n",
       " 0.04776085168123245,\n",
       " 0.04629545286297798,\n",
       " 0.04497610032558441,\n",
       " 0.04375046119093895,\n",
       " 0.04258930683135986,\n",
       " 0.04147614911198616,\n",
       " 0.04040160030126572,\n",
       " 0.03936013579368591,\n",
       " 0.038348421454429626,\n",
       " 0.03736431896686554,\n",
       " 0.03640634939074516,\n",
       " 0.035473428666591644,\n",
       " 0.03456468880176544,\n",
       " 0.03367937356233597,\n",
       " 0.0328168123960495,\n",
       " 0.03197639435529709,\n",
       " 0.03115752898156643,\n",
       " 0.030359644442796707,\n",
       " 0.029582196846604347,\n",
       " 0.028824670240283012,\n",
       " 0.028086528182029724,\n",
       " 0.027367297559976578,\n",
       " 0.026666488498449326,\n",
       " 0.025983627885580063,\n",
       " 0.025318246334791183,\n",
       " 0.024669911712408066,\n",
       " 0.024038175120949745,\n",
       " 0.023422613739967346,\n",
       " 0.02282281592488289,\n",
       " 0.022238384932279587,\n",
       " 0.02166890725493431,\n",
       " 0.02111402526497841,\n",
       " 0.020573347806930542,\n",
       " 0.02004650980234146,\n",
       " 0.019533174112439156,\n",
       " 0.019032973796129227,\n",
       " 0.01854557916522026,\n",
       " 0.018070675432682037,\n",
       " 0.017607925459742546,\n",
       " 0.01715702936053276,\n",
       " 0.01671767793595791,\n",
       " 0.01628957875072956,\n",
       " 0.015872444957494736,\n",
       " 0.015465990640223026,\n",
       " 0.015069940127432346,\n",
       " 0.014684033580124378,\n",
       " 0.01430801022797823,\n",
       " 0.01394161768257618,\n",
       " 0.013584611937403679,\n",
       " 0.013236742466688156,\n",
       " 0.01289778295904398,\n",
       " 0.012567499652504921,\n",
       " 0.01224567461758852,\n",
       " 0.011932093650102615,\n",
       " 0.011626544408500195,\n",
       " 0.011328816413879395,\n",
       " 0.011038717813789845,\n",
       " 0.01075603999197483,\n",
       " 0.010480600409209728,\n",
       " 0.010212220251560211,\n",
       " 0.009950710460543633,\n",
       " 0.009695896878838539,\n",
       " 0.009447610937058926,\n",
       " 0.00920567661523819,\n",
       " 0.008969939313828945,\n",
       " 0.008740244433283806,\n",
       " 0.00851642619818449,\n",
       " 0.008298343978822231,\n",
       " 0.008085845038294792,\n",
       " 0.007878782227635384,\n",
       " 0.007677029352635145,\n",
       " 0.007480439729988575,\n",
       " 0.007288881577551365,\n",
       " 0.007102229632437229,\n",
       " 0.006920357700437307,\n",
       " 0.006743147037923336,\n",
       " 0.006570469122380018,\n",
       " 0.006402213592082262,\n",
       " 0.006238266825675964,\n",
       " 0.006078519392758608,\n",
       " 0.005922866053879261,\n",
       " 0.005771194584667683,\n",
       " 0.0056234076619148254,\n",
       " 0.005479401908814907,\n",
       " 0.005339090246707201,\n",
       " 0.005202369764447212,\n",
       " 0.005069151986390352,\n",
       " 0.004939344711601734,\n",
       " 0.004812859930098057,\n",
       " 0.004689611028879881,\n",
       " 0.004569521173834801,\n",
       " 0.004452507011592388,\n",
       " 0.00433849124237895,\n",
       " 0.004227395635098219,\n",
       " 0.0041191414929926395,\n",
       " 0.004013662226498127,\n",
       " 0.003910883329808712,\n",
       " 0.003810734488070011,\n",
       " 0.0037131512071937323,\n",
       " 0.0036180668976157904,\n",
       " 0.003525417298078537,\n",
       " 0.0034351423382759094,\n",
       " 0.00334717589430511,\n",
       " 0.00326146325096488,\n",
       " 0.0031779457349330187,\n",
       " 0.0030965637415647507,\n",
       " 0.0030172686092555523,\n",
       " 0.002940004225820303,\n",
       " 0.0028647184371948242,\n",
       " 0.0027913597878068686,\n",
       " 0.0027198803145438433,\n",
       " 0.0026502336841076612,\n",
       " 0.0025823668111115694,\n",
       " 0.002516239881515503,\n",
       " 0.0024518056306988,\n",
       " 0.002389021683484316,\n",
       " 0.0023278440348803997,\n",
       " 0.002268234733492136,\n",
       " 0.0022101509384810925,\n",
       " 0.002153555164113641,\n",
       " 0.002098405733704567,\n",
       " 0.002044672379270196,\n",
       " 0.001992313889786601,\n",
       " 0.0019412956899031997,\n",
       " 0.001891584601253271,\n",
       " 0.0018431437201797962,\n",
       " 0.0017959435936063528,\n",
       " 0.0017499554669484496,\n",
       " 0.0017051417380571365,\n",
       " 0.0016614755149930716,\n",
       " 0.0016189299058169127,\n",
       " 0.001577473245561123,\n",
       " 0.0015370768960565329,\n",
       " 0.0014977161772549152,\n",
       " 0.0014593619853258133,\n",
       " 0.0014219910372048616,\n",
       " 0.0013855760917067528,\n",
       " 0.0013500943314284086,\n",
       " 0.0013155194465070963,\n",
       " 0.0012818335089832544,\n",
       " 0.0012490081135183573,\n",
       " 0.0012170245172455907,\n",
       " 0.00118585757445544,\n",
       " 0.001155490754172206,\n",
       " 0.0011259010061621666,\n",
       " 0.0010970698203891516,\n",
       " 0.0010689760092645884,\n",
       " 0.0010416029253974557,\n",
       " 0.0010149299632757902,\n",
       " 0.0009889381472021341,\n",
       " 0.0009636144386604428,\n",
       " 0.0009389390470460057,\n",
       " 0.0009148960816673934,\n",
       " 0.0008914677309803665,\n",
       " 0.0008686375804245472,\n",
       " 0.0008463931735605001,\n",
       " 0.0008247197838500142,\n",
       " 0.0008036002400331199,\n",
       " 0.0007830230752006173,\n",
       " 0.0007629719329997897,\n",
       " 0.0007434340077452362,\n",
       " 0.0007243969012051821,\n",
       " 0.00070584611967206,\n",
       " 0.0006877703126519918,\n",
       " 0.0006701585953123868,\n",
       " 0.00065299600828439,\n",
       " 0.0006362756248563528,\n",
       " 0.0006199817871674895,\n",
       " 0.0006041055894456804,\n",
       " 0.0005886356811970472,\n",
       " 0.0005735635058954358,\n",
       " 0.000558875035494566,\n",
       " 0.000544563343282789,\n",
       " 0.0005306187085807323,\n",
       " 0.0005170313525013626,\n",
       " 0.0005037921364419162,\n",
       " 0.0004908905830234289,\n",
       " 0.00047832049313001335,\n",
       " 0.00046607194235548377,\n",
       " 0.00045413669431582093,\n",
       " 0.00044250726932659745,\n",
       " 0.0004311761003918946,\n",
       " 0.0004201348056085408,\n",
       " 0.00040937718586064875,\n",
       " 0.00039889372419565916,\n",
       " 0.0003886789199896157,\n",
       " 0.0003787254390772432,\n",
       " 0.00036902748979628086,\n",
       " 0.00035957773798145354,\n",
       " 0.0003503700136207044,\n",
       " 0.00034139741910621524,\n",
       " 0.0003326554433442652,\n",
       " 0.00032413724693469703,\n",
       " 0.00031583686359226704,\n",
       " 0.00030774896731600165,\n",
       " 0.00029986744630150497,\n",
       " 0.00029218921554274857,\n",
       " 0.00028470761026255786,\n",
       " 0.00027741678059101105,\n",
       " 0.0002703124191612005,\n",
       " 0.0002633908297866583,\n",
       " 0.00025664549320936203,\n",
       " 0.0002500732953194529,\n",
       " 0.0002436695504002273,\n",
       " 0.0002374293253524229,\n",
       " 0.00023134893854148686,\n",
       " 0.00022542447550222278,\n",
       " 0.0002196522691519931,\n",
       " 0.0002140278520528227,\n",
       " 0.00020854741160292178,\n",
       " 0.00020320728071965277,\n",
       " 0.00019800360314548016,\n",
       " 0.00019293294462841004,\n",
       " 0.00018799275858327746,\n",
       " 0.00018317840294912457,\n",
       " 0.00017848759307526052,\n",
       " 0.00017391644360031933,\n",
       " 0.00016946345567703247,\n",
       " 0.00016512375441379845,\n",
       " 0.0001608958700671792,\n",
       " 0.00015677555347792804,\n",
       " 0.0001527609711047262,\n",
       " 0.00014884857228025794,\n",
       " 0.00014503707643598318,\n",
       " 0.0001413232967024669,\n",
       " 0.0001377040462102741,\n",
       " 0.00013417808804661036,\n",
       " 0.00013074198795948178,\n",
       " 0.00012739365047309548,\n",
       " 0.00012413128570187837,\n",
       " 0.00012095272541046143,\n",
       " 0.0001178552774945274,\n",
       " 0.00011483709386084229,\n",
       " 0.00011189640645170584,\n",
       " 0.000109030821477063,\n",
       " 0.00010623894195305184,\n",
       " 0.00010351857054047287,\n",
       " 0.00010086747352033854,\n",
       " 9.828436304815114e-05,\n",
       " 9.57677184487693e-05,\n",
       " 9.331529145129025e-05,\n",
       " 9.092561231227592e-05,\n",
       " 8.859729132382199e-05,\n",
       " 8.632888784632087e-05,\n",
       " 8.411843737121671e-05,\n",
       " 8.196411363314837e-05,\n",
       " 7.986553100636229e-05,\n",
       " 7.782026659697294e-05,\n",
       " 7.582744001410902e-05,\n",
       " 7.388579251710325e-05,\n",
       " 7.199358515208587e-05,\n",
       " 7.015001028776169e-05,\n",
       " 6.835375097580254e-05,\n",
       " 6.660340295638889e-05,\n",
       " 6.489772931672633e-05,\n",
       " 6.32361407042481e-05,\n",
       " 6.161680357763544e-05,\n",
       " 6.003852467983961e-05,\n",
       " 5.850102752447128e-05,\n",
       " 5.7003271649591625e-05,\n",
       " 5.5543619964737445e-05,\n",
       " 5.4121170251164585e-05,\n",
       " 5.27349766343832e-05,\n",
       " 5.138460983289406e-05,\n",
       " 5.006886931369081e-05,\n",
       " 4.8786780098453164e-05,\n",
       " 4.753740722662769e-05,\n",
       " 4.63204451079946e-05,\n",
       " 4.513421299634501e-05,\n",
       " 4.397823795443401e-05,\n",
       " 4.28523744631093e-05,\n",
       " 4.175518188276328e-05,\n",
       " 4.06858634960372e-05,\n",
       " 3.964406641898677e-05,\n",
       " 3.86290303140413e-05,\n",
       " 3.763969652936794e-05,\n",
       " 3.667592682177201e-05,\n",
       " 3.57367898686789e-05,\n",
       " 3.482175452518277e-05,\n",
       " 3.3930042263818905e-05,\n",
       " 3.306123471702449e-05,\n",
       " 3.221477527404204e-05,\n",
       " 3.138992178719491e-05,\n",
       " 3.058583024539985e-05,\n",
       " 2.9802513381582685e-05,\n",
       " 2.903930544562172e-05,\n",
       " 2.8295715310378e-05,\n",
       " 2.757095717242919e-05,\n",
       " 2.6865200197789818e-05,\n",
       " 2.617698919493705e-05,\n",
       " 2.5506806196062826e-05,\n",
       " 2.485341246938333e-05,\n",
       " 2.421721364953555e-05,\n",
       " 2.3597016479470767e-05,\n",
       " 2.2992564481683075e-05,\n",
       " 2.2403906768886372e-05,\n",
       " 2.1830224795849063e-05,\n",
       " 2.1271225705277175e-05,\n",
       " 2.0726512957480736e-05,\n",
       " 2.0195853721816093e-05,\n",
       " 1.967877324204892e-05,\n",
       " 1.9174804037902504e-05,\n",
       " 1.868388062575832e-05,\n",
       " 1.8205402739113197e-05,\n",
       " 1.7739403119776398e-05,\n",
       " 1.7285046851611696e-05,\n",
       " 1.6842363038449548e-05,\n",
       " 1.6410809621447697e-05,\n",
       " 1.599049210199155e-05,\n",
       " 1.558123403810896e-05,\n",
       " 1.518226599728223e-05,\n",
       " 1.4793502487009391e-05,\n",
       " 1.4414426004805136e-05,\n",
       " 1.4045414900465403e-05,\n",
       " 1.3685727026313543e-05,\n",
       " 1.333524051005952e-05,\n",
       " 1.2993708878639154e-05,\n",
       " 1.2661019354709424e-05,\n",
       " 1.2336719009908848e-05,\n",
       " 1.2020908798149321e-05,\n",
       " 1.1713099411281291e-05,\n",
       " 1.1413183528929949e-05,\n",
       " 1.1121002899017185e-05,\n",
       " 1.0836164619831834e-05,\n",
       " 1.0558474969002418e-05,\n",
       " 1.028807855618652e-05,\n",
       " 1.0024700713984203e-05,\n",
       " 9.768015843292233e-06,\n",
       " 9.517794751445763e-06,\n",
       " 9.273935575038195e-06,\n",
       " 9.036489245772827e-06,\n",
       " 8.805067409412004e-06,\n",
       " 8.579670975450426e-06,\n",
       " 8.360018910025246e-06,\n",
       " 8.145973879436497e-06,\n",
       " 7.937373993627261e-06,\n",
       " 7.734051905572414e-06,\n",
       " 7.536104021710344e-06,\n",
       " 7.343106062762672e-06,\n",
       " 7.155013008741662e-06,\n",
       " 6.971667517063906e-06,\n",
       " 6.793161446694285e-06,\n",
       " 6.619167834287509e-06,\n",
       " 6.449800821428653e-06,\n",
       " 6.284618393692654e-06,\n",
       " 6.123710136307636e-06,\n",
       " 5.966867320239544e-06,\n",
       " 5.814109499624465e-06,\n",
       " 5.665287062583957e-06,\n",
       " 5.520130343938945e-06,\n",
       " 5.378790774557274e-06,\n",
       " 5.241106464382028e-06,\n",
       " 5.106820935907308e-06,\n",
       " 4.976053787686396e-06,\n",
       " 4.848545358981937e-06,\n",
       " 4.724402060674038e-06,\n",
       " 4.603396973834606e-06,\n",
       " 4.48554965259973e-06,\n",
       " 4.370683200249914e-06,\n",
       " 4.258741682861e-06,\n",
       " 4.149626875005197e-06,\n",
       " 4.043312401336152e-06,\n",
       " 3.9398050830641296e-06,\n",
       " 3.8389916880987585e-06,\n",
       " 3.7406280171126127e-06,\n",
       " 3.644805246949545e-06,\n",
       " 3.551497002263204e-06,\n",
       " 3.460524339971016e-06,\n",
       " 3.371945240360219e-06,\n",
       " 3.285539150965633e-06,\n",
       " 3.201438630640041e-06,\n",
       " 3.119418579444755e-06,\n",
       " 3.0395462999877054e-06,\n",
       " 2.9617149266414344e-06,\n",
       " 2.8858407858933788e-06,\n",
       " 2.811942522384925e-06,\n",
       " 2.7399466944189044e-06,\n",
       " 2.669888544915011e-06,\n",
       " 2.601562300696969e-06,\n",
       " 2.5348444978590123e-06,\n",
       " 2.4699231744307326e-06,\n",
       " 2.406712155789137e-06,\n",
       " 2.345035454709432e-06,\n",
       " 2.2849746983411023e-06,\n",
       " 2.2264641756919445e-06,\n",
       " 2.1694495444535278e-06,\n",
       " 2.1139530872460455e-06,\n",
       " 2.0598433820850914e-06,\n",
       " 2.007049261010252e-06,\n",
       " 1.9556900952011347e-06,\n",
       " 1.9056352584811975e-06,\n",
       " 1.856819608292426e-06,\n",
       " 1.809260311347316e-06,\n",
       " 1.7629314470468671e-06,\n",
       " 1.7177960671688197e-06,\n",
       " 1.6737926671339665e-06,\n",
       " 1.6309604689013213e-06,\n",
       " 1.5891985185589874e-06,\n",
       " 1.5485632047784748e-06,\n",
       " 1.508897867097403e-06,\n",
       " 1.4702893622597912e-06,\n",
       " 1.4326357131722034e-06,\n",
       " 1.3959961506770924e-06,\n",
       " 1.360200030831038e-06,\n",
       " 1.3253645647637313e-06,\n",
       " 1.2914715625811368e-06,\n",
       " 1.2583843727043131e-06,\n",
       " 1.2261716619832441e-06,\n",
       " 1.194742253574077e-06,\n",
       " 1.164125478680944e-06,\n",
       " 1.1342694961058442e-06,\n",
       " 1.105239334719954e-06,\n",
       " 1.0769400660137762e-06,\n",
       " 1.0493836271052714e-06,\n",
       " 1.022536366690474e-06,\n",
       " 9.963411002900102e-07,\n",
       " 9.708444395073457e-07,\n",
       " 9.460152341489447e-07,\n",
       " 9.217608862854831e-07,\n",
       " 8.981893984127964e-07,\n",
       " 8.751488849156885e-07,\n",
       " 8.527462114216178e-07,\n",
       " 8.309182248922298e-07,\n",
       " 8.096566261883709e-07,\n",
       " 7.889057656029763e-07,\n",
       " 7.687220886509749e-07,\n",
       " 7.49022547097411e-07,\n",
       " 7.298407354028313e-07,\n",
       " 7.111242439350463e-07,\n",
       " 6.929526534804609e-07,\n",
       " 6.752195531589678e-07,\n",
       " 6.579604132639361e-07,\n",
       " 6.410917876564781e-07,\n",
       " 6.246399379961076e-07,\n",
       " 6.086257826609653e-07,\n",
       " 5.930767201789422e-07,\n",
       " 5.779133971373085e-07,\n",
       " 5.631140993500594e-07,\n",
       " 5.486966188072984e-07,\n",
       " 5.346356601876323e-07,\n",
       " 5.209387268223509e-07,\n",
       " 5.076017259852961e-07,\n",
       " 4.94563778374868e-07,\n",
       " 4.819164587388514e-07,\n",
       " 4.6956671440057107e-07,\n",
       " 4.5753682798022055e-07,\n",
       " 4.458142939256504e-07,\n",
       " 4.344179274085036e-07,\n",
       " 4.232845469687163e-07,\n",
       " 4.124464396682015e-07,\n",
       " 4.0184929162023764e-07,\n",
       " 3.915998263437359e-07,\n",
       " 3.8157992321430356e-07,\n",
       " 3.718209313774423e-07,\n",
       " 3.62304746204245e-07,\n",
       " 3.530116998717858e-07,\n",
       " 3.439569127294817e-07,\n",
       " 3.351437953824643e-07,\n",
       " 3.26559955965422e-07,\n",
       " 3.182011596436496e-07,\n",
       " 3.100635126429552e-07,\n",
       " 3.021233396793832e-07,\n",
       " 2.943822607903712e-07,\n",
       " 2.86848319319688e-07,\n",
       " 2.795078444250976e-07,\n",
       " 2.7234435151513026e-07,\n",
       " 2.653563910826051e-07,\n",
       " 2.585670131338702e-07,\n",
       " 2.5192474595314707e-07,\n",
       " 2.454880245750246e-07,\n",
       " 2.392000624240609e-07,\n",
       " 2.330742319145429e-07,\n",
       " 2.2711463998348336e-07,\n",
       " 2.21286569512813e-07,\n",
       " 2.1563062091445317e-07,\n",
       " 2.101099880746915e-07,\n",
       " 2.0473677864174533e-07,\n",
       " 1.9948909368849854e-07,\n",
       " 1.943752181432501e-07,\n",
       " 1.8939346091428888e-07,\n",
       " 1.845577628500905e-07,\n",
       " 1.7984655187319731e-07,\n",
       " 1.7524368445265281e-07,\n",
       " 1.70753153838632e-07,\n",
       " 1.6639151567687804e-07,\n",
       " 1.6213434150813555e-07,\n",
       " 1.5798627828189638e-07,\n",
       " 1.5394653019029647e-07,\n",
       " 1.5000473752024845e-07,\n",
       " 1.4617074839406996e-07,\n",
       " 1.424297124685836e-07,\n",
       " 1.3877382798455073e-07,\n",
       " 1.352089356032593e-07,\n",
       " 1.317498856678867e-07,\n",
       " 1.2839026908295637e-07,\n",
       " 1.2509477187450102e-07,\n",
       " 1.2187780384920188e-07,\n",
       " 1.1875675909323036e-07,\n",
       " 1.1571725622161466e-07,\n",
       " 1.1273937161604408e-07,\n",
       " 1.0986187248818169e-07,\n",
       " 1.0703581665438833e-07,\n",
       " 1.043039716819294e-07,\n",
       " 1.0162691665982493e-07,\n",
       " 9.901685871227528e-08,\n",
       " 9.648906740267194e-08,\n",
       " 9.400979195106629e-08,\n",
       " 9.160679326214449e-08,\n",
       " 8.926805605824484e-08,\n",
       " 8.697929843037855e-08,\n",
       " 8.47428367478642e-08,\n",
       " 8.256311900822766e-08,\n",
       " 8.0456956652597e-08,\n",
       " 7.839734905701334e-08,\n",
       " 7.638651311481226e-08,\n",
       " 7.443291849540401e-08,\n",
       " 7.251732370150421e-08,\n",
       " 7.06635034930514e-08,\n",
       " 6.884739178758537e-08,\n",
       " 6.708398103683066e-08,\n",
       " 6.536210150898114e-08,\n",
       " 6.368951943613865e-08,\n",
       " 6.206253289064989e-08,\n",
       " 6.047605438652681e-08,\n",
       " 5.8921202139572415e-08,\n",
       " 5.741536313053075e-08,\n",
       " 5.595209628950215e-08,\n",
       " 5.452908524716804e-08,\n",
       " 5.312774931098829e-08,\n",
       " 5.176928041805695e-08,\n",
       " 5.044596207426366e-08,\n",
       " 4.915192164389737e-08,\n",
       " 4.7886011600439815e-08,\n",
       " 4.6657206098643655e-08,\n",
       " 4.5451717056721463e-08,\n",
       " 4.429912792147661e-08,\n",
       " 4.316978419183215e-08,\n",
       " 4.2068542427387e-08,\n",
       " 4.098365025129169e-08,\n",
       " 3.9942054996799925e-08,\n",
       " 3.8910624056143206e-08,\n",
       " 3.7906715988356154e-08,\n",
       " 3.6935709601948474e-08,\n",
       " 3.599131659370869e-08,\n",
       " 3.506541190745338e-08,\n",
       " 3.416207761119949e-08,\n",
       " 3.328513997757909e-08,\n",
       " 3.243560442456328e-08,\n",
       " 3.160315031891514e-08,\n",
       " 3.080038268876706e-08,\n",
       " 3.0018384222785244e-08,\n",
       " 2.923949438127238e-08,\n",
       " 2.849577640517964e-08,\n",
       " 2.775615115524488e-08,\n",
       " 2.705241541889336e-08,\n",
       " 2.6357621862871383e-08,\n",
       " 2.5681899273877207e-08,\n",
       " 2.5022606209290643e-08,\n",
       " 2.4383520980109097e-08,\n",
       " 2.37596360364023e-08,\n",
       " 2.3145309668848313e-08,\n",
       " 2.25512657436866e-08,\n",
       " 2.19756621788747e-08,\n",
       " 2.141466381999635e-08,\n",
       " 2.0866611549763547e-08,\n",
       " 2.0328835503846676e-08,\n",
       " 1.98092600101063e-08,\n",
       " 1.9304106757545014e-08,\n",
       " 1.881221933786037e-08,\n",
       " 1.8326769435361712e-08,\n",
       " 1.7861493617488122e-08,\n",
       " 1.74044352263536e-08,\n",
       " 1.6961504201162825e-08,\n",
       " 1.652568215604333e-08,\n",
       " 1.609953770298489e-08,\n",
       " 1.5691536958684082e-08,\n",
       " 1.5286685695059532e-08,\n",
       " 1.4894534494658274e-08,\n",
       " 1.4510404433565327e-08,\n",
       " 1.4137082615661711e-08,\n",
       " 1.377618730202812e-08,\n",
       " 1.3428397949155624e-08,\n",
       " 1.3082502192673928e-08,\n",
       " 1.2744528099517538e-08,\n",
       " 1.2417848083146055e-08,\n",
       " 1.2099008017685264e-08,\n",
       " 1.1788063858375608e-08,\n",
       " 1.1485112416664833e-08,\n",
       " 1.1190245174930169e-08,\n",
       " 1.090421797300678e-08,\n",
       " 1.0626286517378958e-08,\n",
       " 1.0355401869333036e-08,\n",
       " 1.0091693702918292e-08,\n",
       " 9.83496839523923e-09,\n",
       " 9.58386259242161e-09,\n",
       " 9.337271400511327e-09,\n",
       " 9.095318276308717e-09,\n",
       " 8.861299249929289e-09,\n",
       " 8.636602544243033e-09,\n",
       " 8.413476138002807e-09,\n",
       " 8.198482781551775e-09,\n",
       " 7.987008387999595e-09,\n",
       " 7.783788724680107e-09,\n",
       " 7.584889161194042e-09,\n",
       " 7.392266354599997e-09,\n",
       " 7.201186313920971e-09,\n",
       " 7.017863179470396e-09,\n",
       " 6.838921873253412e-09,\n",
       " 6.66243238356401e-09,\n",
       " 6.4962626389331035e-09,\n",
       " 6.326891455188388e-09,\n",
       " 6.16502138228725e-09,\n",
       " 6.007200958890735e-09,\n",
       " 5.8530269519962985e-09,\n",
       " 5.706956685003206e-09,\n",
       " 5.559339211202996e-09,\n",
       " 5.416779469413768e-09,\n",
       " 5.278676162845386e-09,\n",
       " 5.143491854653348e-09,\n",
       " 5.011583148473164e-09,\n",
       " 4.886022253458577e-09,\n",
       " 4.7604222785935235e-09,\n",
       " 4.639476358647698e-09,\n",
       " 4.520982699318665e-09,\n",
       " 4.405326770040574e-09,\n",
       " 4.292058708443847e-09,\n",
       " 4.1842276310433135e-09,\n",
       " 4.078992699163564e-09,\n",
       " 3.973552153979654e-09,\n",
       " 3.873760423545036e-09,\n",
       " 3.774240475706847e-09,\n",
       " 3.675901361077649e-09,\n",
       " 3.582713459238107e-09,\n",
       " 3.4913287816351612e-09,\n",
       " 3.4019391748074668e-09,\n",
       " 3.316561025812348e-09,\n",
       " 3.2312912345844325e-09,\n",
       " 3.1479250317545393e-09,\n",
       " 3.0682993923392132e-09,\n",
       " 2.989210656778596e-09,\n",
       " 2.9150648561682146e-09,\n",
       " 2.8420228392889157e-09,\n",
       " 2.7674063041160935e-09,\n",
       " 2.695817125086819e-09,\n",
       " 2.6263009544891247e-09,\n",
       " 2.5590680685638745e-09,\n",
       " 2.494175532774534e-09,\n",
       " 2.428310441615622e-09,\n",
       " 2.3681596683644557e-09,\n",
       " 2.3070421129034457e-09,\n",
       " 2.2484230033370523e-09,\n",
       " 2.1897483826194275e-09,\n",
       " 2.134327603542374e-09,\n",
       " 2.0820452029113312e-09,\n",
       " 2.0286836655003526e-09,\n",
       " 1.9763906067282733e-09,\n",
       " 1.9276256146838477e-09,\n",
       " 1.8784984678887895e-09,\n",
       " 1.8314174621281154e-09,\n",
       " 1.7834147492123975e-09,\n",
       " 1.7364290005872363e-09,\n",
       " 1.6910466360542387e-09,\n",
       " 1.6489585252799088e-09,\n",
       " 1.6077066344877267e-09,\n",
       " 1.566713869749492e-09,\n",
       " 1.52615764470454e-09,\n",
       " 1.4883592136527568e-09,\n",
       " 1.4497061329166172e-09,\n",
       " 1.4144332372012514e-09,\n",
       " 1.3768077788967048e-09,\n",
       " 1.3411431964982512e-09,\n",
       " 1.3068539583827032e-09,\n",
       " 1.2750411837458842e-09,\n",
       " 1.2436975893592717e-09,\n",
       " 1.2117244985176967e-09,\n",
       " 1.1805783017848626e-09,\n",
       " 1.150067374666719e-09,\n",
       " 1.120289194744828e-09,\n",
       " 1.0921124005136562e-09,\n",
       " 1.0650464954409244e-09,\n",
       " 1.0371006275988748e-09,\n",
       " 1.011500661007858e-09,\n",
       " 9.845959603183019e-10,\n",
       " 9.584955051877841e-10,\n",
       " 9.337046691371143e-10,\n",
       " 9.106058129759731e-10,\n",
       " 8.871163803547688e-10,\n",
       " 8.651679372917442e-10,\n",
       " 8.435634413217485e-10,\n",
       " 8.221292535637303e-10,\n",
       " 7.998741669013043e-10,\n",
       " 7.800176060612785e-10,\n",
       " 7.605405194510695e-10,\n",
       " 7.413536451394975e-10,\n",
       " 7.22698567656721e-10,\n",
       " 7.038192251229702e-10,\n",
       " 6.859581791474056e-10,\n",
       " 6.688041231939224e-10,\n",
       " 6.515006312213245e-10,\n",
       " 6.355922455014706e-10,\n",
       " 6.197027335730354e-10,\n",
       " 6.044065248289598e-10,\n",
       " 5.894735810585416e-10,\n",
       " 5.737872399436128e-10,\n",
       " 5.592686314059847e-10,\n",
       " 5.444360517969926e-10,\n",
       " 5.300953009879095e-10,\n",
       " 5.164886296427085e-10,\n",
       " 5.033606864657258e-10,\n",
       " 4.903422112789713e-10,\n",
       " 4.784965756954307e-10,\n",
       " 4.665419162108719e-10,\n",
       " 4.548723619990369e-10,\n",
       " 4.430344979766687e-10,\n",
       " 4.3167158736423517e-10,\n",
       " 4.201106129642085e-10,\n",
       " 4.089906191495629e-10,\n",
       " 3.9968450771254993e-10,\n",
       " 3.89913212828219e-10,\n",
       " 3.806785997539919e-10,\n",
       " 3.715028285000699e-10,\n",
       " 3.61885410526952e-10,\n",
       " 3.526581249246874e-10,\n",
       " 3.434681428160502e-10,\n",
       " 3.3436009516663034e-10,\n",
       " 3.259981173897586e-10,\n",
       " 3.17348369804904e-10,\n",
       " 3.0868152478547017e-10,\n",
       " 2.9932722966918845e-10,\n",
       " 2.9229352271897824e-10,\n",
       " 2.8549673736222303e-10,\n",
       " 2.7876767560996996e-10,\n",
       " 2.7184854367590106e-10,\n",
       " 2.6500046601540816e-10,\n",
       " 2.582802860473521e-10,\n",
       " 2.519016106816707e-10,\n",
       " 2.456821412977206e-10,\n",
       " 2.3946800098428866e-10,\n",
       " 2.336428828186854e-10,\n",
       " 2.271274279763702e-10,\n",
       " 2.212372507415239e-10,\n",
       " 2.161766321506775e-10,\n",
       " 2.1075274858617377e-10,\n",
       " 2.0577273218691516e-10,\n",
       " 2.00832905861148e-10,\n",
       " 1.9609469603665275e-10,\n",
       " 1.907700664105505e-10,\n",
       " 1.8607648755164519e-10,\n",
       " 1.8147328084694436e-10,\n",
       " 1.767890278614459e-10,\n",
       " 1.722366693712729e-10,\n",
       " 1.6763768151406566e-10,\n",
       " 1.6284107395847514e-10,\n",
       " 1.588906783922539e-10,\n",
       " 1.5469181491312156e-10,\n",
       " 1.505797708745149e-10,\n",
       " 1.4638867895655494e-10,\n",
       " 1.4250178814734227e-10,\n",
       " 1.3898060480244112e-10,\n",
       " 1.3585954583561488e-10,\n",
       " 1.3258882880506917e-10,\n",
       " 1.2954326500391744e-10,\n",
       " 1.26381349829785e-10,\n",
       " 1.2334289145599087e-10,\n",
       " 1.2065415333495366e-10,\n",
       " 1.1762479878996146e-10,\n",
       " 1.1468848093443285e-10,\n",
       " 1.115549874697308e-10,\n",
       " 1.086977174935555e-10,\n",
       " 1.0589906729308041e-10,\n",
       " 1.0315592824383657e-10,\n",
       " 1.0023804009051673e-10,\n",
       " 9.77309344563082e-11,\n",
       " 9.514122822906756e-11,\n",
       " 9.265632705535154e-11,\n",
       " 9.074097029326822e-11,\n",
       " 8.883604962761638e-11,\n",
       " 8.666600770368404e-11,\n",
       " 8.448863830778919e-11,\n",
       " 8.264522399770158e-11,\n",
       " 8.050893285371785e-11,\n",
       " 7.864753293063131e-11,\n",
       " 7.683076397313471e-11,\n",
       " 7.475198238182656e-11,\n",
       " 7.352740638566502e-11,\n",
       " 7.141842672808707e-11,\n",
       " 6.976041966311186e-11,\n",
       " 6.796518903229298e-11,\n",
       " 6.638711802509079e-11,\n",
       " 6.47213393989432e-11,\n",
       " 6.31272811801864e-11,\n",
       " 6.159850407527756e-11,\n",
       " 5.990496987351435e-11,\n",
       " 5.846523265518044e-11,\n",
       " 5.67217384173091e-11,\n",
       " 5.503020261699021e-11,\n",
       " 5.3695492496785846e-11,\n",
       " 5.2170712194765656e-11,\n",
       " 5.0708992560544175e-11,\n",
       " 4.922284801978094e-11,\n",
       " 4.789502128232925e-11,\n",
       " 4.658806673774052e-11,\n",
       " 4.524203234268498e-11,\n",
       " 4.4169778945502e-11,\n",
       " 4.309930190515843e-11,\n",
       " 4.2420511547902606e-11,\n",
       " 4.143774212650442e-11,\n",
       " 4.056621705217367e-11,\n",
       " 3.960942684955171e-11,\n",
       " 3.884426114098005e-11,\n",
       " 3.797340220046408e-11,\n",
       " 3.7329694890786413e-11,\n",
       " 3.627698141883684e-11,\n",
       " 3.556643868307674e-11,\n",
       " 3.466582576550081e-11,\n",
       " 3.414224458708759e-11,\n",
       " 3.310218765761874e-11,\n",
       " 3.2491120904865056e-11,\n",
       " 3.165001594140904e-11,\n",
       " 3.097744283309112e-11,\n",
       " 2.998978843038458e-11,\n",
       " 2.9497959630475634e-11,\n",
       " 2.8697044740511046e-11,\n",
       " 2.8054003564648156e-11,\n",
       " 2.722888581274674e-11,\n",
       " 2.6372015682341043e-11,\n",
       " 2.578182112245031e-11,\n",
       " 2.506417295933261e-11,\n",
       " 2.4471313864182775e-11,\n",
       " 2.3762547485262075e-11,\n",
       " 2.297495527159299e-11,\n",
       " 2.257860565180181e-11,\n",
       " 2.1760815371862918e-11,\n",
       " 2.1240342817918645e-11,\n",
       " 2.0545121159898372e-11,\n",
       " 2.0222712393547226e-11,\n",
       " 1.982125574784277e-11,\n",
       " 1.911959479627967e-11,\n",
       " 1.8775425658645872e-11,\n",
       " 1.8466117523985304e-11,\n",
       " 1.7875700919489645e-11,\n",
       " 1.7394086171407253e-11,\n",
       " 1.706434993309358e-11,\n",
       " 1.6366019650604358e-11,\n",
       " 1.599675947261403e-11,\n",
       " 1.5722534385531617e-11,\n",
       " 1.52406975928443e-11,\n",
       " 1.4932055591998505e-11,\n",
       " 1.4478862553346517e-11,\n",
       " 1.415068062726732e-11,\n",
       " 1.3504974916145329e-11,\n",
       " 1.329847343356505e-11,\n",
       " 1.2836176566111135e-11,\n",
       " 1.2567502594151847e-11,\n",
       " 1.2235101820579075e-11,\n",
       " 1.2155609852015914e-11,\n",
       " 1.1869394356267549e-11,\n",
       " 1.1725509452276128e-11,\n",
       " 1.1477485628574868e-11,\n",
       " 1.1275202993488165e-11,\n",
       " 1.1009859690602752e-11,\n",
       " 1.0724976462483937e-11,\n",
       " 1.0470957434449701e-11,\n",
       " 1.031641438942188e-11,\n",
       " 1.006617011967137e-11,\n",
       " 9.810152690192808e-12,\n",
       " 9.616307750093256e-12,\n",
       " 9.383605004131823e-12,\n",
       " 9.286127422569734e-12,\n",
       " 9.026557279412373e-12,\n",
       " 8.810951968030167e-12,\n",
       " 8.51096970677645e-12,\n",
       " 8.43725089794134e-12,\n",
       " 8.219203095904959e-12,\n",
       " 7.952749569994921e-12,\n",
       " 7.750022845698368e-12,\n",
       " 7.652323219531354e-12,\n",
       " 7.437828131173774e-12,\n",
       " 7.257749956579573e-12,\n",
       " 7.0812244956641734e-12,\n",
       " 6.930012119710227e-12,\n",
       " 6.692424392440444e-12,\n",
       " 6.547873354634248e-12,\n",
       " 6.326050794314142e-12,\n",
       " 6.213030090407301e-12,\n",
       " 6.030509425158925e-12,\n",
       " 5.920153256511185e-12,\n",
       " 5.725198093387007e-12,\n",
       " 5.5784266095315616e-12,\n",
       " 5.390132784555135e-12,\n",
       " 5.257572155414891e-12,\n",
       " 5.016875803676157e-12,\n",
       " 4.971578704271451e-12,\n",
       " 4.800604358479177e-12,\n",
       " 4.660938301981332e-12,\n",
       " 4.46664927267193e-12,\n",
       " 4.406697229342171e-12,\n",
       " 4.205524817280093e-12,\n",
       " 4.00812716350174e-12,\n",
       " 3.886446720002823e-12,\n",
       " 3.813616089587413e-12,\n",
       " 3.632205647363662e-12,\n",
       " 3.4627856138058632e-12,\n",
       " 3.4279246108326333e-12,\n",
       " 3.2571723096452843e-12,\n",
       " 3.1468161409975437e-12,\n",
       " 3.0182523147459506e-12,\n",
       " 2.9403146584172646e-12,\n",
       " 2.7713387140693158e-12,\n",
       " 2.6845192735436285e-12,\n",
       " 2.5561774918969604e-12,\n",
       " 2.539524146527583e-12,\n",
       " 2.4362734052374435e-12,\n",
       " 2.3916424396475122e-12,\n",
       " 2.312150471084351e-12,\n",
       " 2.254196829198918e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12,\n",
       " 2.1866952693017083e-12]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de090e8-1acb-4769-852a-254a1b10f28a",
   "metadata": {},
   "source": [
    "**After updating two out of three fundamental parts, our current**\n",
    "\n",
    "state of development is:\n",
    "- **Data Preparation V0**\n",
    "- **Model Configuration V1**\n",
    "- **Model Training V1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe467cb6-dacd-48bd-a14c-19fd46b29451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15568a-1100-4b91-81a6-9d447a3c4eae",
   "metadata": {},
   "source": [
    "### create custom dataset (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9f1b59f-a585-4d52-bf8e-03f8cdfa4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1209]), tensor([1.2417]))\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8942c21-d5cd-480c-82da-14c340d34ef9",
   "metadata": {},
   "source": [
    "Did you notice we built our training tensors out of Numpy arrays,\n",
    "\n",
    "but we did not send them to a device? So, they are CPU tensors\n",
    "\n",
    "now! Why?\n",
    "\n",
    "We don’t want our whole training data to be loaded into GPU\n",
    "\n",
    "tensors, as we have been doing in our example so far, because it\n",
    "\n",
    "takes up space in our precious graphics card’s RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4300d-8876-42e3-9f5e-821c009cf944",
   "metadata": {},
   "source": [
    "### TensorDataset (using pytorch dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd54df04-2a8e-410e-a2be-29c221098a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1209]), tensor([1.2417]))\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d96c0c-a69c-4787-ab01-e03434ff8327",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4de55-6adc-49c2-b552-b849da918c21",
   "metadata": {},
   "source": [
    "We tell it which dataset to use\n",
    "(the one we have just built in the previous section), the desired mini-batch size, \n",
    "\n",
    "and\n",
    "if we’d like to shuffle it or not. That’s it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c88cb9-21b2-4154-bb01-aff922925f01",
   "metadata": {},
   "source": [
    "#### IMPORTANT: \n",
    "\n",
    "in the absolute majority of cases, you should set\n",
    "**shuffle=True** \n",
    "for your training set to improve the performance\n",
    "of gradient descent. There are a few exceptions, though, like time\n",
    "series problems, where shuffling actually leads to data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c1aa2-431a-4acb-8207-9d997d927ec8",
   "metadata": {},
   "source": [
    "#### What about the validation and test sets?\n",
    "There is **no need** to\n",
    "shuffle them since **we are not computing gradients with them**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcebfb2-02ed-4586-bc6e-0551b35a6ead",
   "metadata": {},
   "source": [
    "Our **loader** will behave like an **iterator**, so we can **loop over it** and **fetch a different mini-batch** every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f9333-f97a-4a3b-a2f8-83f692f412d6",
   "metadata": {},
   "source": [
    "It is typical to **use powers of two** for mini-batch sizes, like **16, 32, 64 or 128, and 32**\n",
    "seems to be the choice of most people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57846d75-a1d5-491a-b34e-580d346528ab",
   "metadata": {},
   "source": [
    "In our example, we have only 80 training points, so I chose a mini-batch size of 16\n",
    "to conveniently split the training set into five mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07fa707e-acd8-4c5f-aca4-8d2ccdd7c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "dataset=train_data,\n",
    "batch_size=16,\n",
    "shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a83a9-562f-4b54-8790-d892152c48fa",
   "metadata": {},
   "source": [
    "To retrieve a mini-batch, one can simply run the command below — it will return a\n",
    "list containing two tensors, one for the features, another one for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2be8ec-7b7b-4699-aebd-e61d8be9c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3950],\n",
       "         [0.0951],\n",
       "         [0.5758],\n",
       "         [0.7735],\n",
       "         [0.3614],\n",
       "         [0.2897],\n",
       "         [0.9002],\n",
       "         [0.2700],\n",
       "         [0.1209],\n",
       "         [0.5504],\n",
       "         [0.9091],\n",
       "         [0.9118],\n",
       "         [0.2501],\n",
       "         [0.6718],\n",
       "         [0.1992],\n",
       "         [0.3464]]),\n",
       " tensor([[1.7900],\n",
       "         [1.1903],\n",
       "         [2.1516],\n",
       "         [2.5469],\n",
       "         [1.7229],\n",
       "         [1.5795],\n",
       "         [2.8004],\n",
       "         [1.5399],\n",
       "         [1.2417],\n",
       "         [2.1008],\n",
       "         [2.8183],\n",
       "         [2.8237],\n",
       "         [1.5003],\n",
       "         [2.3436],\n",
       "         [1.3984],\n",
       "         [1.6929]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba55978-21de-4365-ae3f-71ad618ddab3",
   "metadata": {},
   "source": [
    "If you call **list(train_loader)**, you’ll get; as a result, a list of 5 elements, that is, all\n",
    "five mini-batches. Then you could take the first element of that list to obtain a\n",
    "single mini-batch as in the example above. It would defeat the purpose of using the\n",
    "iterable provided by the DataLoader, that is, to iterate over the elements (mini-\n",
    "batches, in that case) one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c325d-f62b-4195-9e31-56ef4ce5086d",
   "metadata": {},
   "source": [
    "### change data_preparation1 with dataSet and dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d341c0f-8b82-43f3-9ad9-874833a35ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../chapter1/data_preparation/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/data_preparation/v1.py\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them\n",
    "# into PyTorch's Tensors\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "# Builds Dataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Builds DataLoader\n",
    "train_loader = DataLoader(\n",
    "dataset=train_data,\n",
    "batch_size=16,\n",
    "shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edf8f3d5-d9fb-40cf-b1e4-43845f4c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2b4205d-9bc2-4d51-bd7d-cd3f45dff446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_configuration/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f498284a-7a1c-4727-bec2-b888830ff1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../chapter1/model_training/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_training/v2.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Performs one train step and returns the\n",
    "        # corresponding loss for this mini-batch\n",
    "        mini_batch_loss = train_step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "    \n",
    "    # Computes average loss over all mini-batches\n",
    "    # That's the epoch loss\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20da788b-024e-434f-b218-c62743906e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f173160-42ea-4581-a277-63b309ec2d99",
   "metadata": {},
   "source": [
    "After another two updates, our current state of development is:\n",
    "- **Data Preparation V1**\n",
    "- **Model Configuration V1**\n",
    "- **Model Training V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bccf95f6-480b-41f2-aaf7-cdf2d92cdcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf2971-4ecd-4219-9364-d45767f33a09",
   "metadata": {},
   "source": [
    "### Did you notice it is taking longer to train now? Can you guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059647d5-1365-43bb-9e29-edd4d745a08e",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "the training time is longer now because the inner loop is executed five\n",
    "times for each epoch (in our example, since we are using a mini-batch of size 16 and\n",
    "we have 80 training data points in total, we execute the inner loop 80 / 16 = 5\n",
    "times). \n",
    "\n",
    "So, in total, we are calling the train_step a total of 5,000 times now! No\n",
    "wonder it’s taking longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19fb10ea-7dbc-4a7a-8a83-89bf7bff6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(device, data_loader, step):\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        mini_batch_loss = step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "    \n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2bd468b-d3f1-4d89-a2bd-5281d7c08de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v1.py\n",
    "%run -i ../chapter1/model_configuration/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1abb1a34-2dcd-4f16-8a97-6ae0117378c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../chapter1/model_training/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_training/v3.py\n",
    "\n",
    "#In the last section, we realized that we were executing five times more updates\n",
    "#(the train_step function) per epoch due to the mini-batch inner loop. Before, 1,000\n",
    "#epochs meant 1,000 updates. Now, we only need 200 epochs to perform the same\n",
    "#1,000 updates\n",
    "# Defines number of epochs\n",
    "n_epochs = 200 \n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13e4f142-90ef-4a47-afd2-cc681f2967e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae88af-2ec0-4460-b292-1bf562f2e943",
   "metadata": {},
   "source": [
    "After updating the model training part, our current state of\n",
    "development is:\n",
    "- **Data Preparation V1**\n",
    "- **Model Configuration V1**\n",
    "- **Model Training V3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6febcaca-b9ed-4d8c-80ce-662ba9f6c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c33f3-300d-4ba5-b296-cf5b2a29e894",
   "metadata": {},
   "source": [
    "### Random Split\n",
    "PyTorch’s random_split() method is an easy and familiar way of performing a\n",
    "training-validation split.\n",
    "So far, we’ve been using x_train_tensor and y_train_tensor, built out of the\n",
    "original split in Numpy, to build the training dataset. Now, we’re going to be using\n",
    "the full data from Numpy (x and y), to build a PyTorch Dataset first and only then\n",
    "split the data using random_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8138ffff-d4df-4301-aa3c-73b92ec98047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../chapter1/data_preparation/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/data_preparation/v2.py\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.as_tensor(x).float()\n",
    "y_tensor = torch.as_tensor(y).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Performs the split\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(\n",
    "dataset=train_data,\n",
    "batch_size=16,\n",
    "shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ca4364d-6c10-41e3-8eff-7eaed46396c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4631dde-cb92-4500-bc07-52c95a58e86e",
   "metadata": {},
   "source": [
    "### evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3900fee-dfe2-410f-af58-0169c87888a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step(model, loss_fn):\n",
    "    # Builds function that performs a step\n",
    "    # in the validation loop\n",
    "    def perform_val_step(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output\n",
    "        # forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # There is no need to compute Steps 3 and 4,\n",
    "        # since we don't update parameters during evaluation\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "713269ff-e716-40e7-9dcf-8968a69b2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../chapter1/model_configuration/v2.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile ../chapter1/model_configuration/v2.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Creates the train_step function for our model, loss function\n",
    "# and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# Creates the val_step function for our model and loss function\n",
    "val_step = make_val_step(model, loss_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c30b0cb-0d4d-4eec-a3cc-dd0a630c5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_configuration/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87464787-4f5d-46b0-a05d-1c74216bafe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../chapter1/model_training/v4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_training/v4.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "# VALIDATION - no gradients in validation!\n",
    "with torch.no_grad():\n",
    "    val_loss = mini_batch(device, val_loader, val_step)\n",
    "    val_losses.append(val_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9efa0626-cfb0-4f02-9da5-b9d5961ea0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeee242-3c70-496a-a57b-d838ec2148e8",
   "metadata": {},
   "source": [
    "After updating all parts, in sequence, our current state of\n",
    "development is:\n",
    "- **Data Preparation V2**\n",
    "- **Model Configuration V2**\n",
    "- **Model Training V4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e84aa32-44b8-4f87-b214-3d181b3f1d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c627b8c-2791-4651-b8e2-e83eddaba82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dad260af-47a4-4ec8-ab6e-142fa67e5e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-de8962d0d8886ee2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-de8962d0d8886ee2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs #you can run it separately in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62101b3c-9c79-488e-bab9-98e38960e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97a448-6ca3-4d79-a520-e5dddc83aa17",
   "metadata": {},
   "source": [
    "If we do not specify any folder, TensorBoard will default to\n",
    "runs/CURRENT_DATETIME_HOSTNAME, which is not such a great\n",
    "name if you’d be looking for your experiment results in the\n",
    "future.\n",
    "\n",
    "So, it is recommended to try to name it in a more meaningful\n",
    "way, like runs/test or runs/simple_linear_regression. It will\n",
    "then create a subfolder inside runs (the folder we specified when\n",
    "we started TensorBoard).\n",
    "\n",
    "Even better, you should name it in a meaningful way and add\n",
    "datetime or a sequential number as a suffix, like runs/test_001\n",
    "or runs/test_20200502172130, to avoid writing data of multiple\n",
    "runs into the same folder (we’ll see why this is bad in the\n",
    "add_scalars section below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dad60-ce97-4b67-930e-8c12f9f23805",
   "metadata": {},
   "source": [
    "The summary writer implements several methods to allow us sending information to TensorBoard:\n",
    "\n",
    "- **add_graph**\n",
    "- **add_scalars**\n",
    "- **add_scalar**\n",
    "- **add_histogram**\n",
    "- **add_images**\n",
    "- **add_image**\n",
    "- **add_figure**\n",
    "- **add_video**\n",
    "- **add_audio**\n",
    "- **add_text**\n",
    "- **add_embedding**\n",
    "- **add_pr_curve**\n",
    "- **add_custom_scalars**\n",
    "- **add_mesh add_hparams**\n",
    "- \n",
    "It also implements two other methods for effectively writing data to disk:\n",
    "- **flush**\n",
    "- **close**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "460ec453-a673-421a-a4fa-258d355bd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching a tuple of feature (dummy_x) and label (dummy_y)\n",
    "dummy_x, dummy_y = next(iter(train_loader))\n",
    "# Since our model was sent to device, we need to do the same\n",
    "# with the data.\n",
    "# Even here, both model and data need to be on the same device!\n",
    "writer.add_graph(model, dummy_x.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba2498-9fae-4072-b76e-e76ffc0bb93f",
   "metadata": {},
   "source": [
    "### add_scalars\n",
    "\n",
    "What about sending the loss values to TensorBoard? I’m on it! We can use\n",
    "\n",
    "*add_scalars* method to send multiple scalar values at once, and it needs three\n",
    "arguments:\n",
    "\n",
    "- **main_tag:** the parent name of the tags or, the \"group tag\", if you will\n",
    "- **tag_scalar_dict:** the dictionary containing the key: value pairs for the scalars\n",
    "\n",
    "you want to keep track of (in our case, training and validation losses)\n",
    "- **global_step:** step value, that is, the index you’re associating with the values\n",
    "you’re sending in the dictionary - the epoch comes to mind in our case, as losses\n",
    "are computed for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ccf84c1f-2804-4cfc-af80-413944dbd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalars(\n",
    "main_tag='loss',\n",
    "tag_scalar_dict={'training': loss,\n",
    "'validation': val_loss},\n",
    "global_step=epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e654e87-8703-4a1e-a1f2-7aef087b7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83248500-fe03-4979-88e7-56f1a126fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../chapter1/model_configuration/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_configuration/v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Creates the train_step function for our model,\n",
    "# loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# Creates the val_step function for our model and loss function\n",
    "val_step = make_val_step(model, loss_fn)\n",
    "\n",
    "# Creates a Summary Writer to interface with TensorBoard\n",
    "writer = SummaryWriter('runs/simple_linear_regression')\n",
    "# Fetches a single mini-batch so we can use add_graph\n",
    "x_dummy, y_dummy = next(iter(train_loader))\n",
    "writer.add_graph(model, x_dummy.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "763e072e-95e3-4ea5-a525-ba07b2ebbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_configuration/v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afd1f8b7-056e-43d7-9fde-c74d5393019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../chapter1/model_training/v5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../chapter1/model_training/v5.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # VALIDATION - no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    # Records both losses for each epoch under tag \"loss\"\n",
    "    writer.add_scalars(main_tag='loss',\n",
    "                        tag_scalar_dict={\n",
    "                        'training': loss,\n",
    "                        'validation': val_loss},\n",
    "                        global_step=epoch)\n",
    "\n",
    "# Closes the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5dbc6b74-8028-4d3a-8500-6c48645bd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4166195-32e7-4abd-bb52-e7163a92e2bc",
   "metadata": {},
   "source": [
    "After the last update of both model configuration and training\n",
    "parts, our current state of development is:\n",
    "- **Data Preparation V2**\n",
    "- **Model Configuration V3**\n",
    "- **Model Training V5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e77b6466-3cc4-4946-aae3-1942a7e6e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace0816-15f1-4353-8448-3defc256e7f6",
   "metadata": {},
   "source": [
    "**If our model**\n",
    "**were big or complex enough to take at least a couple of minutes to train, we would**\n",
    "**be able to see the evolution of our losses in TensorBoard during training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebedc5-99e4-426d-9385-ed70e51b12f4",
   "metadata": {},
   "source": [
    "### **Saving and Loading Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa83a-4b85-466a-8da8-4917dcbdd4f9",
   "metadata": {},
   "source": [
    "Training a model successfully is great, no doubt about that, but not all models will\n",
    "be that fast to be trained, and maybe training gets interrupted (computer crashing,\n",
    "timeout after 12h of continuous GPU usage on Google Colab, etc.).\n",
    "\n",
    "It would be a pity to have to start over\n",
    "\n",
    "So, it is important to be able to checkpoint or save our model, \n",
    "\n",
    "that is, saving it to\n",
    "disk, in case we’d like to restart training later or deploy it as an application to make\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ab161-d591-4187-b5e2-f3944bae42ef",
   "metadata": {},
   "source": [
    "### Saving\n",
    "Now, wrap everything into a Python dictionary and use **torch.save()** to dump it\n",
    "all into a file. \n",
    "\n",
    "Easy peasy! We have just saved our model to a file named\n",
    "**model_checkpoint.pth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02246d5d-15db-48fe-a1df-c6853fad13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': n_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': losses,\n",
    "            'val_loss': val_losses}\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c053b40-3484-4ad0-bbcf-b451ea503191",
   "metadata": {},
   "source": [
    "#### **Resuming Training**\n",
    "If we’re starting fresh (as if we had just turned on the computer and started\n",
    "Jupyter), we have to set up the stage before actually loading the model. \n",
    "\n",
    "This means\n",
    "we need to load the data and configure the model.\n",
    "    \n",
    "Luckily, we have code for that already: **Data Preparation V2** and **Model Configuration V3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2410ecf9-5673-4d7b-85ee-86ac2a8e6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/data_preparation/v2.py\n",
    "%run -i ../chapter1/model_configuration/v3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca8831-128f-4139-9d86-aa500a9b6137",
   "metadata": {},
   "source": [
    "#### let’s double-check that we do have an **untrained model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db5ff400-8927-4479-9931-2dd025ecf24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')), ('0.bias', tensor([0.8300], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8c175-6850-44de-bcf0-d482249f1ad1",
   "metadata": {},
   "source": [
    "Now we are ready to load the model back, which is easy:\n",
    "- **load the dictionary back using torch.load()**\n",
    "- **load model and optimizer state dictionaries back using the load_state_dict()\n",
    "method**\n",
    "- **load everything else into their corresponding variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19d9ee22-397b-41f2-bf09-67ffb8429f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "saved_epoch = checkpoint['epoch']\n",
    "saved_losses = checkpoint['loss']\n",
    "saved_val_losses = checkpoint['val_loss']\n",
    "model.train() # always use TRAIN for resuming training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba3ec7b1-37d1-4bc2-8f14-5c98f61f9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01069135-b633-4c19-9fcf-96036d0a8695",
   "metadata": {},
   "source": [
    "#### **Next, we can run **Model Training V5** to train it for another 200 epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d64e3ba6-7345-49c0-a5e8-0417c1cb8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_training/v5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19802fca-21ba-4b7a-b382-b735b78b1bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7343c-1562-4b75-afbc-66b892715fbf",
   "metadata": {},
   "source": [
    "Well, it didn’t change at all, which is no surprise: the original model had converged\n",
    "already; that is, the loss was at a minimum. \n",
    "\n",
    "These extra epochs served an\n",
    "educational purpose only; they did not improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eb956-3577-450c-94af-66e1d03c07df",
   "metadata": {},
   "source": [
    "### **Deploying / Making Predictions**\n",
    "\n",
    "Again, if we’re starting fresh (as if we had just turned on the computer and started\n",
    "Jupyter), we have to set up the stage before actually loading the model.\n",
    "\n",
    "But, this\n",
    "time, we only need to configure the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adfd797c-1e14-4e2f-a92d-43d02b2866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../chapter1/model_configuration/v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e5a769a-0110-4bda-82da-16a6f990dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4204c632-1a57-43a2-aa6b-8afee4a434ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4000],\n",
       "        [1.6800],\n",
       "        [2.1400]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the model\n",
    "new_inputs = torch.tensor([[.20], [.34], [.57]])\n",
    "model.eval() # always use EVAL for fully trained models!\n",
    "model(new_inputs.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9170a1-7a35-46c3-bd36-030c485469d4",
   "metadata": {},
   "source": [
    "**After loading a fully trained model for deployment / to make predictions, make sure you ALWAYS set it to evaluation mode:**\n",
    "\n",
    "\n",
    "**model.eval()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07597f97-ad3a-4ad1-9779-119bb781589e",
   "metadata": {},
   "source": [
    "# **Putting It All Together**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1edc687-51e9-4455-aabd-3463f5efc7c8",
   "metadata": {},
   "source": [
    "your pipeline: **Data Preparation V2**, **Model Configuration V3**, and **Model Training V5**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df9cd8a3-84d6-482d-9185-70ca3f197e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_preparation/v2.py\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.as_tensor(x).float()\n",
    "y_tensor = torch.as_tensor(y).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Performs the split\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(\n",
    "                dataset=train_data,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                )\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33249489-4913-488c-8f78-3c09537c89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_configuration/v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Creates the train_step function for our model,\n",
    "# loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# Creates the val_step function for our model and loss function\n",
    "val_step = make_val_step(model, loss_fn)\n",
    "\n",
    "# Creates a Summary Writer to interface with TensorBoard\n",
    "writer = SummaryWriter('runs/simple_linear_regression')\n",
    "# Fetches a single mini-batch so we can use add_graph\n",
    "x_dummy, y_dummt = next(iter(train_loader))\n",
    "writer.add_graph(model, x_dummy.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8be4f03f-9d3d-4234-9751-13e93b8d0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_training/v5.py\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # VALIDATION - no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    # Records both losses for each epoch under tag \"loss\"\n",
    "    writer.add_scalars(main_tag='loss',\n",
    "    tag_scalar_dict={\n",
    "            'training': loss,\n",
    "            'validation': val_loss},\n",
    "            global_step=epoch)\n",
    "\n",
    "# Closes the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d1545d2-6f18-47fd-843e-4013d30f3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[2.0000]], device='cuda:0')), ('0.bias', tensor([1.0000], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2be9b1-9a67-4616-af69-f4734198fd09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
